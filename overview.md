This research project, submitted to the ICSA 2026 Software Architecture SAGAI Workshop, extends an existing edge-based standing-water detection system for off-road agricultural vehicles.

The original architecture employs Raspberry Pi-class devices (with optional NVIDIA Jetson acceleration) to process RGB camera frames and environmental sensor data (humidity, temperature, pressure) via a finite-state machine (FSM) that orchestrates adaptive model tiering, multi-model YOLO consensus, and diurnal sensor fusion for real-time flood hazard detection under resource constraints.

To enhance usability and decision support, we integrate a generative large language model (LLM) as a modular addition to the architecture. The system features a fixed dashboard that displays interactive visualizations synthesized from detection logs, sensor anomalies, and FSM decisions. As the vehicle traverses fields, the system produces real-time maps overlaid with color-coded regions: red for confirmed floods (class 2), yellow for suspicious areas (class 1, "Some Water"), and green for no flood (class 0). Maps include geolocated pins at flood sites; clicking a pin displays the corresponding detection image alongside info detailing hazard severity, environmental context, temporal trends, and mitigation suggestions.

Stakeholder-specific reports are generated via LLM prompts tailored to user needs: farmers receive operational summaries focusing on mobility risks and crop impacts; agronomists get analytical reports on soil health and anomaly patterns; farm managers/operators focus on vehicle safety, detection logs, and metrics; government agencies (e.g., agriculture, environment, disaster management) obtain compliance overviews with audit trails; insurance companies get damage assessments with evidence. This LLM extension preserves the system's determinism and replayability while leveraging generative AI for adaptive, human-centered reporting, evaluated through hardware-in-the-loop simulations on diverse field scenarios.
